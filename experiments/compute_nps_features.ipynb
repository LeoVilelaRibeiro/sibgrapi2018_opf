{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from utils import data_reader\n",
    "from utils import processing\n",
    "from utils import opf_helper\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total of 7743 samples\n",
      "INFO:root:Statement - 2338\n",
      "INFO:root:System - 1714\n",
      "INFO:root:Greet - 1202\n",
      "INFO:root:Emotion - 849\n",
      "INFO:root:ynQuestion - 402\n",
      "INFO:root:whQuestion - 379\n",
      "INFO:root:Bye - 162\n",
      "INFO:root:Accept - 155\n",
      "INFO:root:Continuer - 124\n",
      "INFO:root:Emphasis - 122\n",
      "INFO:root:Reject - 104\n",
      "INFO:root:yAnswer - 80\n",
      "INFO:root:nAnswer - 55\n",
      "INFO:root:Clarify - 30\n",
      "INFO:root:Other - 27\n",
      "INFO:root:Total of 1412 samples\n",
      "INFO:root:System - 535\n",
      "INFO:root:Statement - 442\n",
      "INFO:root:Emotion - 101\n",
      "INFO:root:Greet - 88\n",
      "INFO:root:whQuestion - 57\n",
      "INFO:root:ynQuestion - 53\n",
      "INFO:root:Accept - 35\n",
      "INFO:root:Emphasis - 23\n",
      "INFO:root:Bye - 22\n",
      "INFO:root:Reject - 19\n",
      "INFO:root:yAnswer - 12\n",
      "INFO:root:nAnswer - 10\n",
      "INFO:root:Continuer - 8\n",
      "INFO:root:Clarify - 5\n",
      "INFO:root:Other - 2\n",
      "INFO:root:Total of 1412 samples\n",
      "INFO:root:Statement - 405\n",
      "INFO:root:System - 383\n",
      "INFO:root:Emotion - 156\n",
      "INFO:root:whQuestion - 97\n",
      "INFO:root:ynQuestion - 95\n",
      "INFO:root:Greet - 73\n",
      "INFO:root:Emphasis - 45\n",
      "INFO:root:Accept - 43\n",
      "INFO:root:Reject - 36\n",
      "INFO:root:Continuer - 36\n",
      "INFO:root:yAnswer - 16\n",
      "INFO:root:Bye - 11\n",
      "INFO:root:nAnswer - 7\n",
      "INFO:root:Other - 6\n",
      "INFO:root:Clarify - 3\n",
      "INFO:gensim.utils:loading EuclideanKeyedVectors object from ../vsms/wglove.840B.300d.bin\n",
      "INFO:gensim.utils:loading syn0 from ../vsms/wglove.840B.300d.bin.syn0.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute syn0norm to None\n",
      "INFO:gensim.utils:loaded ../vsms/wglove.840B.300d.bin\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# /\\ hidding Stanford Parser warning messages\n",
    "\n",
    "TOK_PATH    = '../tokenizer/stanford-corenlp-3.9.0.jar'\n",
    "MODEL_PATH  = '../vsms/wglove.840B.300d.bin'\n",
    "\n",
    "DATASET_T = '../datasets/clean/nps_train.tsv'\n",
    "DATASET_D = '../datasets/clean/nps_dev.tsv'\n",
    "DATASET_E = '../datasets/clean/nps_test.tsv'\n",
    "\n",
    "FEATURES_FILE = './nps_opf/nps_samples.txt'\n",
    "MIN_WORD_FREQ = 1\n",
    "\n",
    "X_train, y_train = data_reader.read_dataset(DATASET_T)\n",
    "X_dev,   y_dev   = data_reader.read_dataset(DATASET_D)\n",
    "X_test,  y_test  = data_reader.read_dataset(DATASET_E)\n",
    "\n",
    "model     = KeyedVectors.load(MODEL_PATH)\n",
    "tokenizer = StanfordTokenizer(TOK_PATH)\n",
    "\n",
    "X_tok_t, word_freq = processing.tokenize_stanford(X_train, tokenizer)\n",
    "X_tok_d, _         = processing.tokenize_stanford(X_dev, tokenizer)\n",
    "X_tok_e, _         = processing.tokenize_stanford(X_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rbp_rpar(tokenized):\n",
    "    new_tok = list()\n",
    "    for sentence in tokenized:\n",
    "        new_sentence = list()\n",
    "        for word in sentence:\n",
    "            word = word.replace('-RRB-', ')')\n",
    "            new_sentence.append(word)\n",
    "        new_tok.append(new_sentence)\n",
    "        \n",
    "    return new_tok\n",
    "\n",
    "X_tok_t = replace_rbp_rpar(X_tok_t)\n",
    "X_tok_d = replace_rbp_rpar(X_tok_d)\n",
    "X_tok_e = replace_rbp_rpar(X_tok_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Vocab size:   4024\n",
      "INFO:root:Keeping:      3885 (96.55%)\n",
      "INFO:gensim.models.keyedvectors:precomputing L2-norms of word weight vectors\n",
      "WARNING:root:212 sentences without representation.\n",
      "WARNING:root:446 OOV words\n",
      "WARNING:root:79 sentences without representation.\n",
      "WARNING:root:528 OOV words\n",
      "WARNING:root:145 sentences without representation.\n",
      "WARNING:root:515 OOV words\n"
     ]
    }
   ],
   "source": [
    "pruned_vocab = processing.keep_common_words(word_freq, MIN_WORD_FREQ)\n",
    "\n",
    "X_emb_t = processing.tok_sentence_to_vec(X_tok_t, pruned_vocab, model, normalize_sentence=4,\n",
    "                                         normalize_word=False, show_logs=1)\n",
    "X_emb_d = processing.tok_sentence_to_vec(X_tok_d, pruned_vocab, model, normalize_sentence=4,\n",
    "                                         normalize_word=False, show_logs=1)\n",
    "X_emb_e = processing.tok_sentence_to_vec(X_tok_e, pruned_vocab, model, normalize_sentence=4,\n",
    "                                         normalize_word=False, show_logs=1)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_emb_t = encoder.fit_transform(y_train)\n",
    "y_emb_d = encoder.transform(y_dev)\n",
    "y_emb_e = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([X_emb_t, X_emb_d, X_emb_e])\n",
    "y = np.vstack([y_emb_t[:, None], y_emb_d[:, None], y_emb_e[:, None]]).flatten()\n",
    "\n",
    "# writing all features file, so distances can be computed\n",
    "opf_helper.write_opf_format(\n",
    "    X,\n",
    "    y + 1, # OPF indices start at 1\n",
    "    FEATURES_FILE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
