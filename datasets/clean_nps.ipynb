{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import random\n",
    "from os import path, getcwd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import nps_chat\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     /Users/lzfelix/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The corpus needs to be downloaded once\n",
    "nltk.download('nps_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATASET_FILES = path.join(getcwd(), './nps_chat/*.xml')\n",
    "# This code uses the dataset that can be obtained through NLTK\n",
    "\n",
    "TRAIN_PATH = path.join(getcwd(), './clean/nps_train.tsv')\n",
    "DEV_PATH = path.join(getcwd(), './clean/nps_dev.tsv')\n",
    "TEST_PATH = path.join(getcwd(), './clean/nps_test.tsv')\n",
    "\n",
    "TRAIN_SET = ['10-19-adults_706posts.xml', '11-09-20s_706posts.xml', '11-08-20s_705posts.xml', '10-19-40s_686posts.xml', '11-09-40s_706posts.xml', '10-24-40s_706posts.xml', '11-09-teens_706posts.xml', '11-08-40s_706posts.xml', '10-19-30s_705posts.xml', '11-08-adults_705posts.xml', '11-06-adults_706posts.xml']\n",
    "DEV_SET   = ['10-26-teens_706posts.xml', '11-09-adults_706posts.xml']\n",
    "TEST_SET  = ['10-19-20s_706posts.xml', '11-08-teens_706posts.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    TOKEN_EMOJI = '[emj]'\n",
    "    TOKEN_USERNAME = '[usr]'\n",
    "    TOKEN_NUMBER = '[num]'\n",
    "    \n",
    "    TOKEN_EMOJI = 'EMOJI_WORD'\n",
    "    TOKEN_USERNAME = 'user'\n",
    "    TOKEN_NUMBER = ''\n",
    "    TOKEN_EMPTY = 'XXX'\n",
    "    \n",
    "    def __init__(self, room_name, replace_emoji=False):\n",
    "        self.username_regex = re.compile(r\"(\\d{1,2}\\-){2}\" + room_name + \"(User\\d+)\")\n",
    "        self.emoji_regex = re.compile(r'(>?[:;=\\+]-?[P\\)\\(@\\*o>]|<3|o_0|0o|o0|>_>|o_o)')\n",
    "        self.url_regex = re.compile(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n",
    "        self.replace_emoji = replace_emoji\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Removes ellipses regardless of their size and username mentions.\"\"\"\n",
    "        \n",
    "        text = re.sub(self.url_regex, 'url', text)\n",
    "        \n",
    "        # Replaces username mention and emojis by tokens\n",
    "        text = re.sub(self.emoji_regex, self.TOKEN_EMOJI, text)\n",
    "\n",
    "        text = re.sub(self.username_regex, self.TOKEN_USERNAME, text)\n",
    "        \n",
    "        # It seems that there's a bug on the dataset. During the anonymization\n",
    "        # phase sentences such as 'USER ACTION ...' had the user part replaced\n",
    "        # by '.'\n",
    "        text = re.sub('^.ACTION', self.TOKEN_USERNAME + ' ACTION', text)\n",
    "        \n",
    "        # Removes non-alphanumerics, usual punctuation and ellipsis\n",
    "        text = re.sub(r'[\\[\\]\\'!\\^\\\\\\.\\(\\)\\*\\/%,\\-\"#@]', ' ', text)\n",
    "        text = re.sub(r'\\.{3}', ' ', text)\n",
    "        text = re.sub(r'!{2,}', '!', text)\n",
    "        text = re.sub(r'\\?{2,}', '?', text)\n",
    "        \n",
    "        # If replacing by number before, the tokenizer will create a new word for it\n",
    "        text = re.sub('\\d+', self.TOKEN_NUMBER, text)\n",
    "        \n",
    "        # Removing redundant spaces and lowercasing\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)\n",
    "        \n",
    "        # adding the emoji back, if this is the case\n",
    "        if not self.replace_emoji:\n",
    "            text = re.sub(self.TOKEN_EMOJI, ':)', text)\n",
    "\n",
    "        text = re.sub(r'_+', '', text)\n",
    "        text = text.lower().strip()\n",
    "        if len(text) == 0:\n",
    "            text = self.TOKEN_EMPTY\n",
    "            \n",
    "        if '_' in text:\n",
    "            print(text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_split_data(split_files):\n",
    "    samples = list()\n",
    "    \n",
    "    for file_id in split_files:\n",
    "        common_name = file_id.split('.')[0]\n",
    "        \n",
    "        # all usernames on this chatroom will contain this substring. This is\n",
    "        # used to replace all mentions to a specific token\n",
    "        username_substring = file_id.split('-')[2].split('_')[0]\n",
    "        pre = Preprocessor(username_substring)\n",
    "        \n",
    "        for i, utterance in enumerate(nps_chat.xml_posts(file_id)):\n",
    "            text = utterance.text\n",
    "            \n",
    "            samples.append({\n",
    "                'id': '{}_{}'.format(file_id, str(i+1)),\n",
    "                'label': utterance.attrib['class'],\n",
    "                'text':  text,\n",
    "                'clean': pre.preprocess(text)\n",
    "            })\n",
    "            \n",
    "    return samples\n",
    "\n",
    "\n",
    "train_set = load_split_data(TRAIN_SET)\n",
    "dev_set = load_split_data(DEV_SET)\n",
    "test_set = load_split_data(TEST_SET)\n",
    "n_conversations = len(train_set) + len(dev_set) + len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split summary:\n",
      " - Train: 7743\t73.28%\n",
      " - Dev:   1412\t13.36%\n",
      " - Test:  1412\t13.36%\n",
      " - Total: 10567\n"
     ]
    }
   ],
   "source": [
    "print('Split summary:')\n",
    "print(' - Train: {}\\t{:.4}%'.format(len(train_set), len(train_set) / n_conversations * 100))\n",
    "print(' - Dev:   {}\\t{:.4}%'.format(len(dev_set), len(dev_set) / n_conversations * 100))\n",
    "print(' - Test:  {}\\t{:.4}%'.format(len(test_set), len(test_set) / n_conversations * 100))\n",
    "print(' - Total: {}'.format(n_conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Persisting data\n",
    "\n",
    "def write_partition_to_disk(dataset, destination_file):\n",
    "    \"\"\"Creates the train/dev/test file with the utterances from all dialogues in <dataset>.\"\"\"\n",
    "\n",
    "    with open(destination_file, 'w') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerow(['id', 'label', 'text', 'clean'])\n",
    "        \n",
    "        for sample in dataset:\n",
    "            writer.writerow([sample['id'], sample['label'], sample['text'], sample['clean']])\n",
    "\n",
    "\n",
    "write_partition_to_disk(train_set, TRAIN_PATH)\n",
    "write_partition_to_disk(dev_set, DEV_PATH)\n",
    "write_partition_to_disk(test_set, TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-label-   \t-clean text (40 chars)-                 \n",
      "Greet       \thello user                                                                                \t\n",
      "Statement   \tlook at us babi look at us now                                                            \t\n",
      "System      \tjoin                                                                                      \t\n",
      "System      \tjoin                                                                                      \t\n",
      "Emotion     \tlol                                                                                       \t\n",
      "Greet       \tuser                                                                                      \t\n",
      "Statement   \tnevermind                                                                                 \t\n",
      "Other       \tntmn                                                                                      \t\n",
      "System      \tjoin                                                                                      \t\n",
      "Statement   \tf az                                                                                      \t\n",
      "System      \tuser action watches user kiss user s big toe                                              \t\n",
      "Greet       \thiya user                                                                                 \t\n",
      "Statement   \thmmm cyber sex sucks                                                                      \t\n",
      "System      \tpart                                                                                      \t\n",
      "Emotion     \tlol user                                                                                  \t\n",
      "Reject      \tnot me ever                                                                               \t\n",
      "Statement   \ti only talk the ppl i know already in my pm                                               \t\n",
      "Bye         \tnight user                                                                                \t\n",
      "Statement   \tshe s the exception                                                                       \t\n",
      "Greet       \thi user                                                                                   \t\n",
      "Greet       \tuser                                                                                      \t\n",
      "Emotion     \tlol user                                                                                  \t\n",
      "System      \tjoin                                                                                      \t\n",
      "Statement   \tahhhh memories                                                                            \t\n",
      "Greet       \tuser hiya                                                                                 \t\n",
      "Emotion     \tholy crap                                                                                 \t\n",
      "Statement   \tum                                                                                        \t\n",
      "System      \tuser action looks for user in the shower                                                  \t\n",
      "System      \tjoin                                                                                      \t\n",
      "Greet       \thi user                                                                                   \t\n",
      "Greet       \thi user                                                                                   \t\n",
      "Statement   \tcali is better                                                                            \t\n",
      "System      \tpart                                                                                      \t\n",
      "Greet       \thi user                                                                                   \t\n",
      "Emotion     \tuser                                                                                      \t\n",
      "Greet       \they wb user                                                                               \t\n",
      "Emotion     \t:tongue:                                                                                  \t\n",
      "Statement   \tpm me                                                                                     \t\n",
      "Greet       \thi guys                                                                                   \t\n",
      "Greet       \twelome to my room                                                                         \t\n",
      "Statement   \t<user your easy to beat up                                                                \t\n",
      "Statement   \tim a single parent to i seem to date boys and im the mama                                 \t\n",
      "Statement   \ti still wait on ssi ssid hearin none yer                                                  \t\n",
      "Statement   \tpokes user didn t want to b left out                                                      \t\n",
      "Accept      \tyeah user haha its proper                                                                 \t\n",
      "Statement   \twell enjoy then user                                                                      \t\n",
      "Greet       \tello                                                                                      \t\n",
      "Statement   \tla la la                                                                                  \t\n",
      "Bye         \tnite user                                                                                 \t\n",
      "Greet       \thi user                                                                                   \t\n"
     ]
    }
   ],
   "source": [
    "# Data peeking\n",
    "\n",
    "AMOUNT_PEEKS = 50\n",
    "\n",
    "sampled_utterances = [random.randint(0, len(train_set) - 1) for _ in range(AMOUNT_PEEKS)]\n",
    "\n",
    "print('{:10.10}\\t{:40.40}'.format('-label-', '-clean text (40 chars)-'))\n",
    "for i in sampled_utterances:\n",
    "    \n",
    "    label = train_set[i]['label'] \n",
    "    text = train_set[i]['clean'] \n",
    "    \n",
    "    print('{:12.12}\\t{:90.90}\\t'.format(label, text))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras_env)",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
