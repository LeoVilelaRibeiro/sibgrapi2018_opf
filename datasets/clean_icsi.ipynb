{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Parses the ICSI dialogue corpus into transcript files with DA annotations. The\n",
    "    transcribed text is already preprocessed, hence this script just parses it,\n",
    "    generating a more friendly tsv file.\n",
    "    \n",
    "    The original dataset has 2083 unique DAs, but they can be mapped into a smaller\n",
    "    collection of labels by using the maps provided on the classmaps/ folder.\n",
    "    \n",
    "    On this notebook we map the 2083 classes into 5 classes, namely: D (disruption),\n",
    "    B (backchannel), F (filler), S (statement), Q (question). The Z  class (non-\n",
    "    labelled) is removed from the clean dataset.\n",
    "    \n",
    "    [CONTENTS]\n",
    "        - Parsing\n",
    "        - Data peeking\n",
    "        - Label occurrence statistics\n",
    "        - Splitting data into train/dev/test\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "from os import path, getcwd, remove\n",
    "from random import random\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# the dataset transcriptions folder and transcription extension\n",
    "DATASET_PATH = path.join(getcwd(), 'icsi/data/')\n",
    "DATASET_ORIGINAL_EXT = '*.dadb'\n",
    "\n",
    "# the class map to downsample 200+ to 5 classes\n",
    "CLASS_MAP_PATH = 'icsi/classmaps/map_01b_expanded'\n",
    "\n",
    "# write split files with punctuation\n",
    "TRAIN_PUNCT_PATH  = path.join(getcwd(), 'clean/icsi_train.tsv')\n",
    "DEV_PUNCT_PATH    = path.join(getcwd(), 'clean/icsi_dev.tsv')\n",
    "TEST_PUNCT_PATH   = path.join(getcwd(), 'clean/icsi_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split used\n",
    "train_set_idx = ['Bdb001', 'Bed002', 'Bed004', 'Bed005', 'Bed008', 'Bed009', 'Bed011', 'Bed013', 'Bed014', 'Bed015', 'Bed017', 'Bmr002', 'Bmr003', 'Bmr006', 'Bmr007', 'Bmr008', 'Bmr009', 'Bmr011', 'Bmr012', 'Bmr015', 'Bmr016', 'Bmr020', 'Bmr021', 'Bmr023', 'Bmr025', 'Bmr026', 'Bmr027', 'Bmr029', 'Bmr031', 'Bns001', 'Bns002', 'Bns003', 'Bro003', 'Bro005', 'Bro007', 'Bro010', 'Bro012', 'Bro013', 'Bro015', 'Bro016', 'Bro017', 'Bro019', 'Bro022', 'Bro023', 'Bro025', 'Bro026', 'Bro028', 'Bsr001', 'Btr001', 'Btr002', 'Buw001']\n",
    "dev_set_idx   = ['Bed003', 'Bed010', 'Bmr005', 'Bmr014', 'Bmr019', 'Bmr024', 'Bmr030', 'Bro004', 'Bro011', 'Bro018', 'Bro024']\n",
    "test_set_idx  =  ['Bed006', 'Bed012', 'Bed016', 'Bmr001', 'Bmr010', 'Bmr022', 'Bmr028', 'Bro008', 'Bro014', 'Bro021', 'Bro027']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \n",
    "    # these transcriptions are removed according to the dataset recommendations\n",
    "    BLACKLIST = ['Bmr013.dadb', 'Bmr018.dadb']\n",
    "    \n",
    "    def __init__(self, amount_peeks=100):\n",
    "        self.classmap = self._load_classmap()\n",
    "        self.samples = list()\n",
    "        self.freq = defaultdict(int)\n",
    "        self.amount_peeks = amount_peeks\n",
    "        \n",
    "    def clean(self, text):\n",
    "        text = re.sub(r'\\{\\}', '', text)\n",
    "        text = re.sub(r'(\\w)-', r'\\g<1> ', text)\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)\n",
    "        \n",
    "        # i_d -> id, x_m_l -> xml, i_seventeen -> iseveteen\n",
    "        text = re.sub(r'[_\\-<>{}\\|\\.]', '', text)\n",
    "        return text.replace('@reject@', '').strip()\n",
    "        \n",
    "    def _join_fragmented_text(self, text):\n",
    "        \"\"\"\n",
    "        Transforms the timed utterances from the .dadb file format into\n",
    "        a regular string.\n",
    "        \"\"\"\n",
    "        \n",
    "        nodes = text.split('|')\n",
    "        nodes = [node[node.rfind('+')+1:] for node in nodes]\n",
    "        text = ' '.join(nodes)\n",
    "        return self.clean(text)\n",
    "    \n",
    "    def _load_classmap(self):\n",
    "        \"\"\"\n",
    "        Maps the original tags to 7 tags according to the map_01_expanded:\n",
    "        D (disruption), B (backchannel), F (filler), S (statement), Q\n",
    "        (question) and Z (non-labelled). Note that the X tag is maped to\n",
    "        itself, but it is usually associated with utterances removed from\n",
    "        the dataset, as they are associated with some transcription error\n",
    "        code (ie: bleeped line, no words).\n",
    "        \"\"\"\n",
    "\n",
    "        map_path = path.join(getcwd(), CLASS_MAP_PATH)\n",
    "\n",
    "        classmap = OrderedDict()\n",
    "        with open(map_path, 'r') as file:\n",
    "            reader = csv.reader(file, delimiter='\\t')\n",
    "            for line in reader:\n",
    "                label, new_label = line\n",
    "                classmap[label] = new_label\n",
    "\n",
    "        return classmap\n",
    "    \n",
    "    def read_trans_file(self, path):\n",
    "        table = dict()\n",
    "        \n",
    "        with open(path) as file:\n",
    "            reader = csv.reader(file)\n",
    "            for line in reader:\n",
    "                sentence_id = line[0]\n",
    "                punct_sentence = line[-1]\n",
    "                \n",
    "                table[sentence_id] = punct_sentence\n",
    "        \n",
    "        return table\n",
    "\n",
    "    def preprocess(self, origin_path, destination_path, punctuation_file=None):\n",
    "        \"\"\"\n",
    "        Reads a .dadb utterance file, extract the necessary information and\n",
    "        generates a new tsv file that can be used for DA classification. The\n",
    "        generated tsv file contains the fields id, original label, mapped label,\n",
    "        error_code, speaker_id and the text.\n",
    "\n",
    "        The transcribed text is mostly preprocessed, so we just format it into\n",
    "        a regular string.\n",
    "        \"\"\"\n",
    "        \n",
    "        if path.basename(origin_path) in self.BLACKLIST:\n",
    "            return\n",
    "        \n",
    "        if punctuation_file:\n",
    "            punct_table = self.read_trans_file(punctuation_file)\n",
    "            \n",
    "        dest = open(destination_path, 'w')\n",
    "        writer = csv.writer(dest, delimiter='\\t')\n",
    "        writer.writerow(['id', 'original_label', 'label', 'errcode', 'speaker', 'clean'])\n",
    "        \n",
    "        with open(origin_path, 'r') as origin:\n",
    "            reader = csv.reader(origin, delimiter=',')\n",
    "            \n",
    "            # See doc/database-format.txt for details about the indices\n",
    "            for line in reader:\n",
    "                utterance_id = line[2]\n",
    "                error_code = line[3]\n",
    "                label = line[5]\n",
    "                speaker = line[7]\n",
    "                original_label = line[8]\n",
    "                \n",
    "                if not punctuation_file:\n",
    "                    fragmented_text = line[4]\n",
    "                    text = self._join_fragmented_text(fragmented_text)\n",
    "                else:\n",
    "                    text = punct_table.get(utterance_id)\n",
    "                    if not text:\n",
    "                        print('----> COULD NOT FIND SENTENCE <-----')\n",
    "\n",
    "                text = self.clean(text)\n",
    "\n",
    "                # see line 1771 of the used classmap\n",
    "                if label == 's^e.%-:s.%--':\n",
    "                    label = 's^e.%-:s.%-'\n",
    "\n",
    "                label_ = self.classmap.get(label, label)\n",
    "                \n",
    "                # skipping error and not-labelled (always related to error-marked utterances)\n",
    "                if error_code in ['B', 'D', 'Z']:\n",
    "                    continue\n",
    "                \n",
    "                # Z = not-labelled, so it is skipped\n",
    "                if label_ in [' ', '', 'Z']:\n",
    "                    continue\n",
    "                    \n",
    "                writer.writerow([utterance_id, original_label, label_, error_code, speaker, text])\n",
    "\n",
    "                self.freq[label_] += 1\n",
    "                self.samples.append((label_, text))\n",
    "                    \n",
    "        dest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_transcriptions(destination_writer, origin_file_path):\n",
    "    \"\"\"Moves transcription parsed lines from one file to another.\"\"\"\n",
    "    \n",
    "    with open(origin_file_path, 'r') as origin:\n",
    "        reader = csv.reader(origin, delimiter='\\t')\n",
    "        for i, line in enumerate(reader):\n",
    "            # skipping the header\n",
    "            if i == 0:\n",
    "                continue\n",
    "                \n",
    "            destination_writer.writerow(line)\n",
    "\n",
    "            \n",
    "def merge_files(destination_path, origin_files_base_path, filenames):\n",
    "    \"\"\"Puts all samples from a split into a single file.\"\"\"\n",
    "    \n",
    "    i = 0\n",
    "    with open(destination_path, 'w') as dest_file:\n",
    "        writer = csv.writer(dest_file, delimiter='\\t')\n",
    "        writer.writerow(['id', 'original_label', 'label', 'errcode', 'speaker', 'clean'])\n",
    "        \n",
    "        for filename in filenames:\n",
    "            i+=1\n",
    "            file_path = path.join(origin_files_base_path, filename + '.tsv')\n",
    "            merge_transcriptions(writer, file_path)\n",
    "\n",
    "\n",
    "def generate_dataset_splits(train_set, dev_set, test_set, dest_train,\n",
    "                            dest_dev, dest_test, use_punctuation=False):\n",
    "    \"\"\"Merges multiple transcript files into a single file representing the dataset.\"\"\"\n",
    "    \n",
    "    parser = Parser()\n",
    "    \n",
    "    # for each transcription in the dataset folder extract its transcriptions\n",
    "    for transcription in glob.glob(path.join(DATASET_PATH, DATASET_ORIGINAL_EXT)):\n",
    "        filename = path.basename(transcription).split('.')[0] \n",
    "        destin_file = path.join(getcwd(), DATASET_PATH, filename + '.tsv')\n",
    "        \n",
    "        if use_punctuation:\n",
    "            punctuation_file = transcription.split('.')[0] + '.trans'\n",
    "        else:\n",
    "            punctuation_file = None\n",
    "            \n",
    "        # reads the dadb (optionally the trans) file and generates a formatted\n",
    "        # transcription file\n",
    "        parser.preprocess(transcription, destin_file, punctuation_file)\n",
    "        \n",
    "    # merging and persisting transcription files into split files\n",
    "    merge_files(dest_train, DATASET_PATH, train_set)\n",
    "    merge_files(dest_dev, DATASET_PATH, dev_set)\n",
    "    merge_files(dest_test, DATASET_PATH, test_set)\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating data splits with punctuation\n",
    "parser = generate_dataset_splits(train_set_idx, dev_set_idx, test_set_idx,\n",
    "                                 TRAIN_PUNCT_PATH, DEV_PUNCT_PATH,\n",
    "                                 TEST_PUNCT_PATH,\n",
    "                                 True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-label-\t-original text (90 chars)-                                                                \n",
      "S      \tok  let's be done with this                                                               \t\n",
      "S      \tok                                                                                        \t\n",
      "S      \tok                                                                                        \t\n",
      "D      \tthis is ami who ==                                                                        \t\n",
      "S      \tand this is tilman and ralf                                                               \t\n",
      "S      \thi                                                                                        \t\n",
      "S      \tuh huh  nice to meet you                                                                  \t\n",
      "S      \thi                                                                                        \t\n",
      "S      \thi                                                                                        \t\n",
      "S      \tok                                                                                        \t\n",
      "S      \tso we're gonna try to finish by five so people who want to can go hear nancy chang's talk \t\n",
      "B      \thmm                                                                                       \t\n",
      "S      \tand you guys are g  giving talks on tomorrow and wednesday lunch times                    \t\n",
      "S      \tyes                                                                                       \t\n",
      "B      \tmmm                                                                                       \t\n",
      "Q      \tright ?                                                                                   \t\n",
      "S      \tthat's great                                                                              \t\n",
      "Q      \tok so do y  do you know what we're gonna do ?                                             \t\n",
      "S      \ti thought two things                                                                      \t\n",
      "S      \tuh  we'll introduce ourselves and what we do                                              \t\n",
      "S      \tand um  we already talked with andreas thilo and david                                    \t\n",
      "S      \tand some lines of code were already written today                                         \t\n",
      "S      \tand almost tested                                                                         \t\n",
      "S      \tand just gonna say we have um  again the recognizer to parser thing where we're working on\t\n",
      "S      \tand that should be no problem                                                             \t\n",
      "S      \tand then that can be sort of developed uh  as needed when we get  enter the tourism domain\t\n",
      "S      \tem  we have talked this morning with the  with tilman about the generator                 \t\n",
      "S      \tand um  there one of our diligent workers has to sort of volunteer to look over tilman's s\t\n",
      "D      \ts ==                                                                                      \t\n",
      "B      \tmm hmm                                                                                    \t\n"
     ]
    }
   ],
   "source": [
    "# data peeking\n",
    "print('{:7.7}\\t{:90.90}'.format('-label-', '-original text (90 chars)-'))\n",
    "for sample in parser.samples[:30]:\n",
    "    print('{:7.7}\\t{:90.90}\\t'.format(sample[0], sample[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S            62589\n",
      "D            14814\n",
      "B            14164\n",
      "F            7683\n",
      "Q            6797\n",
      "Total        106047\n"
     ]
    }
   ],
   "source": [
    "# Label frequencies\n",
    "\n",
    "freqs = sorted(parser.freq.items(), key=lambda x: x[1], reverse=True)\n",
    "for label, freq in freqs:\n",
    "    print('{:12.12} {}'.format(label, freq))\n",
    "    \n",
    "print('{:12.12} {}'.format('Total', sum([v for _, v in freqs]))) #109"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras_env)",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
